{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38cc98af",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1738e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"LoRA\" # Change the Input to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5eb2935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.30s/it]\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Joshua\\.cache\\huggingface\\hub\\models--markteammate--Mistral_academic_style_tune. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Write an academic paragraph given the title.\n",
      "\n",
      "### Input: LoRA\n",
      "\n",
      "### Response:\n",
      "We present LoRA (Loosely Regularized Adapters), a novel approach to fine-tune large language models (LLMs) for in-context learning tasks. Unlike conventional fine-tuning methods that modify the model's weights, LoRA only adds a few hundred additional trainable parameters to the model, reducing the risk of overfitting and maintaining the model's core functionality. Our experiments show that LoRA achieves comparable performance to conventional fine-tuning, while requiring significantly fewer trainable parameters. LoRA also offers additional benefits such as faster training times, reduced memory requirements, and better generalization to unseen data. We demonstrate the effectiveness of LoRA on a variety of in-context learning tasks, including\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# === Config ===\n",
    "base_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "lora_repo = \"Joshua-Sun-CompSci/Mistral_academic_style_tune\"\n",
    "\n",
    "# === Tokenizer === \n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_repo)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === QLoRA quantization config ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# === Load base model with quantization ===\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# === Load LoRA adapter from Hugging Face ===\n",
    "model = PeftModel.from_pretrained(base_model, lora_repo)\n",
    "model.eval()\n",
    "\n",
    "# === Prompt ===\n",
    "prompt = \"\"\"### Instruction:\n",
    "Write an academic paragraph given the title.\n",
    "\n",
    "### Input: \"\"\" + input + \"\"\"\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# === Tokenize and generate ===\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "# === Decode ===\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
